{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# シグナリングゲーム（ポリアの壺）\n",
        "\n",
        "このノートブックでは，ポリアの壺モデルを用いた2体の強化学習エージェント（送信者と受信者）を同時に学習させて，シグナリングシステムが獲得される過程を観察する実験を行います．\n",
        "\n",
        "まず，必要なパッケージをインポートします．この実験では，ほとんど外部のライブラリを使いませんので，インポートするのは `numpy` だけです．"
      ],
      "metadata": {
        "id": "De2HkHqkre4W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzIwhMmVg5u2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "rng = np.random.default_rng()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，シグナリングゲームを表現する強化学習環境（`SignalingGameEnv`）を定義します．\n",
        "状態数（`num_states`），信号数（`num_messages`），状態の確率分布（`state_dist`）を指定して環境を作成できるようにしています．\n",
        "環境の初期化を行う `__init__` メソッドの他に，以下の3つのメソッドを定義しています．\n",
        "\n",
        "- `reset`: 最初からゲームプレイを行えるように環境をリセットします．この時，送信者に与えられる状態がランダムに決定されます．\n",
        "- `observe`: エージェントに与えられる観測を返します．\n",
        "- `step`: エージェントが選択した行動を受け取って，ゲームを進めます．"
      ],
      "metadata": {
        "id": "5Evauj16rdnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignalingGameEnv:\n",
        "    def __init__(self, num_states, num_messages, state_dist):\n",
        "        self.num_states = num_states\n",
        "        self.num_messages = num_messages\n",
        "        self.state_dist = state_dist\n",
        "        self.agents = ['sender', 'receiver']\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = rng.choice(self.num_states)\n",
        "        self.done = False\n",
        "        self.agent_selection = 'sender'\n",
        "\n",
        "    def observe(self, agent):\n",
        "        if agent == 'sender':\n",
        "            return self.state\n",
        "        elif agent == 'receiver':\n",
        "            return self.message\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.done == False, 'WARNING: The game is done. Call reset().'\n",
        "        self.rewards = {'sender': 0.0, 'receiver': 0.0}\n",
        "        agent = self.agent_selection\n",
        "        if agent == 'sender':\n",
        "            self.message = action\n",
        "            self.agent_selection = 'receiver'\n",
        "        elif agent == 'receiver':\n",
        "            self.action = action\n",
        "            if self.state == action:\n",
        "                self.rewards['sender'] = 1.0\n",
        "                self.rewards['receiver'] = 1.0\n",
        "            self.done = True"
      ],
      "metadata": {
        "id": "YIrsOemzhZiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，ポリアの壺モデルに基づく強化学習エージェントのクラス（`UrnAgent`）を定義します．\n",
        "観測の種類数（`num_obs`，送信者の場合は状態数，受信者の場合は信号数に対応）と，行動の種類数（`num_actions`，送信者の場合は信号数，受信者の場合は状態数に対応）を指定して作成できるようにしています．初期化を行う `__init__` メソッドの他に，以下の4つのメソッドを定義しています．\n",
        "\n",
        "- `get_action`: 観測を受け取って，行動を選択して返します．\n",
        "- `store_buffer`: 観測と状態のペアを受け取って，経験として訓練バッファに保存します．\n",
        "- `update_reward`: 次の自分のターンが回ってくる前に，他のエージェントの行動によって自分が報酬を受け取れる場合があります（例えばシグナリングゲームの場合，送信者が行動した時点では報酬はありませんが，次に受信者が適切な行動をした場合には，送信者にも報酬が与えられます）．これを，直近の行動に対する報酬とみなして，訓練バッファの内容を更新します．\n",
        "- `train`: 現在の訓練バッファの内容を使って，方策を更新します．"
      ],
      "metadata": {
        "id": "xu9QTPUryyNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UrnAgent:\n",
        "    def __init__(self, num_obs, num_actions):\n",
        "        self.num_obs = num_obs\n",
        "        self.num_actions = num_actions\n",
        "        self.urn_balls = [np.ones(num_actions, dtype=float) for _ in range(num_obs)]\n",
        "        self.urn_sum_balls = [num_actions for _ in range(num_obs)]\n",
        "        self.train_buf = []\n",
        "\n",
        "    def get_action(self, obs):\n",
        "        p = self.urn_balls[obs] / self.urn_sum_balls[obs]\n",
        "        return rng.choice(np.arange(self.num_actions), p=p)\n",
        "\n",
        "    def store_buffer(self, obs, action):\n",
        "        self.train_buf.append([obs, action, 0]) # 観測，行動，報酬の組\n",
        "\n",
        "    def update_reward(self, reward):\n",
        "        if len(self.train_buf) > 0:\n",
        "          self.train_buf[-1][2] += reward\n",
        "\n",
        "    def train(self):\n",
        "        for obs, action, reward in self.train_buf:\n",
        "            self.urn_balls[obs][action] += reward\n",
        "            self.urn_sum_balls[obs] += reward\n",
        "        self.train_buf = []"
      ],
      "metadata": {
        "id": "4Eg0SJxZiWVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルで，具体的に環境と強化学習モデルを定義します．\n",
        "環境のパラメータを変えて実験してみましょう．"
      ],
      "metadata": {
        "id": "gZIdvgZD1qae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_states = 2          # 状態数\n",
        "num_messages = 2        # 信号数\n",
        "# 状態の確率分布\n",
        "state_dist = [1.0 / num_states] * num_states\n",
        "# state_dist = [0.5, 0.5]\n",
        "env = SignalingGameEnv(num_states, num_messages, state_dist)\n",
        "\n",
        "sender = UrnAgent(env.num_states, env.num_messages)\n",
        "receiver = UrnAgent(env.num_messages, env.num_states)\n",
        "agents = {'sender': sender, 'receiver': receiver}\n",
        "\n",
        "total_timesteps = 20000  # ゲームプレイ回数\n",
        "n_steps = 5             # 訓練データバッファのサイズ"
      ],
      "metadata": {
        "id": "ZTLmKnJahLxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードで，2体の強化学習モデルの学習を行います．"
      ],
      "metadata": {
        "id": "94lyg8G45Ox_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_rewards = {'sender': [], 'receiver': []}  # あとで学習過程を確認するためのログ\n",
        "for timestep in range(total_timesteps):\n",
        "    # ゲームプレイ開始\n",
        "    env.reset()\n",
        "    while not env.done:\n",
        "        agent_name = env.agent_selection\n",
        "        obs = env.observe(agent_name)\n",
        "        action = agents[agent_name].get_action(obs)\n",
        "        env.step(action)\n",
        "        agents[agent_name].store_buffer(obs, action)\n",
        "        for _agent_name in env.agents:\n",
        "            reward = env.rewards[_agent_name]\n",
        "            agents[_agent_name].update_reward(reward)\n",
        "    # ゲームプレイ終了．方策を更新する\n",
        "    if (timestep + 1) % n_steps == 0:\n",
        "        for _agent_name in env.agents:\n",
        "            mean_reward = np.mean([r for o, a, r in agents[_agent_name].train_buf])\n",
        "            log_rewards[_agent_name].append(mean_reward) # ロギング\n",
        "            agents[_agent_name].train()"
      ],
      "metadata": {
        "id": "sXL4unAWhG1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードで，壺の中に入っているボールの数を確認する．"
      ],
      "metadata": {
        "id": "R072kypL5YpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print agent's urn\n",
        "for agent_name in env.agents:\n",
        "    agent = agents[agent_name]\n",
        "    obs_name, action_name = ('state', 'message') if agent_name == 'sender' else ('message', 'action')\n",
        "    print(f'{agent_name} urn:')\n",
        "    for obs, balls in enumerate(agent.urn_balls):\n",
        "        print('{0:>7} {1:>2}:'.format(obs_name, obs))\n",
        "        for action, n in enumerate(balls):\n",
        "            print('  {0:>7} {1:>2}: {2:>6}'.format(action_name, action, n))\n",
        "        print()"
      ],
      "metadata": {
        "id": "aMc4N3Qf4d80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードで，学習した結果の方策を用いたときの，ゲームプレイごとの平均報酬を評価します．\n",
        "コミュニケーションが成功すれば報酬1，失敗すれば報酬0なので，平均報酬の値は，コミュニケーションの成功率ということになります．"
      ],
      "metadata": {
        "id": "wgoyKPA07rRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate policy\n",
        "n_eval_episodes = 100\n",
        "sender_rewards = []\n",
        "receiver_rewards = []\n",
        "for _ in range(n_eval_episodes):\n",
        "    env.reset()\n",
        "    while not env.done:\n",
        "        agent_name = env.agent_selection\n",
        "        obs = env.observe(agent_name)\n",
        "        action = agents[agent_name].get_action(obs)\n",
        "        env.step(action)\n",
        "    sender_rewards.append(env.rewards['sender'])\n",
        "    receiver_rewards.append(env.rewards['receiver'])\n",
        "print(f'Sender average reward: {np.mean(sender_rewards):.2f}')\n",
        "print(f'Receiver average reward: {np.mean(receiver_rewards):.2f}')"
      ],
      "metadata": {
        "id": "sNgOdHsd4gQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコードでは，学習過程における報酬のログをTensorboardで確認できる形式で保存している．"
      ],
      "metadata": {
        "id": "muPfYM_XEPy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "settings = f'states{env.num_states}_messages{env.num_messages}_state_dist{env.state_dist}'\n",
        "writer = SummaryWriter(log_dir=f'./tb/{settings}')\n",
        "for i, (r_s, r_r) in enumerate(zip(log_rewards['sender'], log_rewards['receiver'])):\n",
        "    writer.add_scalar(\"sender_reward\", r_s, i * n_steps)\n",
        "    writer.add_scalar(\"receiver_reward\", r_r, i * n_steps)\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "Jgfn06-FhNeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のコマンドでTensorboardが起動する．\n",
        "このセルは一度だけ実行し，再実行しないことをお勧めする．\n",
        "新しく追加したデータを再読み込みしたい場合，Tensorboardのコンソール上のリロードボタン（右上の方にあるボタン）を押してデータの再読み込みを行うようにするとよい．"
      ],
      "metadata": {
        "id": "RsTPk_ypJNuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./tb"
      ],
      "metadata": {
        "id": "1_058joe-JZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 演習課題\n",
        "\n",
        "- 状態数（`num_states`）と信号数（`num_messages`）を大きくして実験を行い，コミュニケーションの成功率がどのように変化するか調べてみよう．また，その時の壺の中のボールの分布を観察して，なぜコミュニケーションの成功率が変化したのか考察してみよう．\n",
        "- 状態数が2，信号数が2のシグナリングゲームで，状態の確率が偏った設定（`[0.7, 0.3]` や `[0.8, 0.2]`，`[0.9, 0.1]` など）で実験を行い，コミュニケーションの成功率がどのように変化するか調べてみよう．また，その時の壺の中のボールの分布を観察して，なぜコミュニケーションの成功率が変化したのか考察してみよう．\n",
        "- 信号数を，状態数とは異なる数に設定して実験を行い，コミュニケーションの成功率がどのように変化するか調べてみよう．信号数が状態数よりも小さい場合と，大きい場合で，それぞれ壺の中のボールの分布がどのように変化するか観察して，結果の理由を考察してみよう．"
      ],
      "metadata": {
        "id": "EznA0Z37F_AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TFw-RuFk_spc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}